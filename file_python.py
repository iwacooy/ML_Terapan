# -*- coding: utf-8 -*-
"""Untitled49.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1THzMg6_uTAQNUlm8uhgrBM1hLbSrwDil

#Import Library
Melakukan import library yang dibutuhkan
"""

import kagglehub
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import recall_score
from sklearn.metrics import precision_score

"""Mengunduh dataset"""

!curl -L -o /content/bank-marketing.zip https://www.kaggle.com/api/v1/datasets/download/janiobachmann/bank-marketing-dataset

!unzip bank-marketing.zip

"""#Read Data

Memahami data
"""

df = pd.read_csv('bank.csv')
df.head()

df.info()

"""##Check missing value"""

df.isnull().sum()

"""##Check outliers
Pengecekan outliers, mendeteksi nilai-nilai outlier (di luar batas normal).Outlier dapat mempengaruhi distribusi data dan membuat model bias
"""

num_feature = df.select_dtypes(['int64']).columns
num_feature

for col in num_feature:
  print(f"{col}")
  Q1 = df[col].quantile(0.25)
  Q3 = df[col].quantile(0.75)
  IQR = Q3 - Q1
  fence_high = Q3 + 1.5*IQR
  fence_low = Q1 - 1.5*IQR
  outliers = df[(df[col] < fence_low) | (df[col] > fence_high)]
  print(outliers.shape)

for col in num_feature:
  print(f"Removing outliers from col {col}")
  Q1 = df[col].quantile(0.25)
  Q3 = df[col].quantile(0.75)
  IQR = Q3 - Q1
  fence_high = Q3 + 1.5*IQR
  fence_low = Q1 - 1.5*IQR
  outliers = df[(df[col] < fence_low) | (df[col] > fence_high)]
  df = df[(df[col] >= fence_low) & (df[col] <= fence_high)]

df.shape

"""##Check duplicate"""

df.duplicated().sum()

"""#EDA

##Univariate Analysis
analisis statistik yang fokus hanya pada satu variabel dalam satu waktu. Tujuannya adalah untuk memahami karakteristik dasar dari variabel tersebut, baik numerik maupun kategorikal.
"""

df.describe()

num_feature = num_feature.tolist()

plt.figure(figsize=(15, 7))
for i in range(len(num_feature)):
  plt.subplot(2,4, i+1)
  sns.histplot(x = num_feature[i], data = df)
  plt.tight_layout()

"""#Multivariate Analysis
teknik analisis data yang melibatkan dua atau lebih variabel sekaligus. Tujuannya adalah untuk melihat bagaimana variabel-variabel tersebut berinteraksi satu sama lain, apakah ada korelasi, pola, atau pengaruh antar fitur.
"""

sns.pairplot(df[num_feature])

sns.heatmap(df[num_feature].corr(), annot = True)

"""#Additional Data Preparation before Modeling

##Feature Engineering
Dikarenakan kurangnya korelasi antar variabel, maka dilakukan beberapa teknik Feature Engineering pada deposit, dan memberikan feature baru bernama deposit_group untuk mengubah dan menyempurnakan fitur agar dapat meningkatkan performa model machine learning.
"""

df["balance_group"] = "positive"

df.loc[df["balance"] < 0 , "balance_group"] = "negative"

df.loc[df["balance"] == 0 , "balance_group"] = "empty"

"""##One-Hot Encoding
memungkinkan model untuk memproses informasi kategorikal tanpa memberikan urutan atau bobot yang salah pada kategori tersebut.
"""

obj_feature = df.select_dtypes(["object"]).columns.tolist()

obj_feature.remove("deposit")

df = pd.get_dummies(df, columns = obj_feature)

"""##Train test split"""

X = df.drop(columns = ["deposit"])
y = df["deposit"].map({"yes": 1, "no": 0})

X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3, random_state=42)

print("Total data: ", X_train.shape)
print("Total data: ", X_test.shape)
print("Total data: ", y_train.shape)
print("Total data: ", y_test.shape)

"""#Modeling
Berdasarkan EDA, data memiliki distribusi yang tidak normal, sehingga digunakan algoritma yang tidak mengasumsikan distribusi normal, seperti Decision Tree dan Random Forest.

##Decission Tree
"""

dtree = DecisionTreeClassifier()
dtree.fit(X_train, y_train)

"""##Random Forest"""

rf = RandomForestClassifier()
rf.fit(X_train, y_train)

"""##Random Forest Hyperparameter Tuning"""

params = {
    'max_depth': [2,3,5,10,15],
    'min_samples_leaf': [10,20,25,35,50]
}

grid_search = GridSearchCV(estimator = RandomForestClassifier(),
                           param_grid = params,
                           cv = 5,
                           scoring = 'roc_auc'
                           )
grid_search.fit(X_train, y_train)

rf_best = grid_search.best_estimator_
rf_best.fit(X_train,y_train)

"""#Evaluasi

##Recall
"""

y_dtree = dtree.predict(X_test)
y_rf = rf.predict(X_test)
y_rf_best = rf_best.predict(X_test)

print('DT: ',recall_score(y_test, y_dtree, pos_label = 1, average = 'binary'))
print('RF: ',recall_score(y_test, y_rf, pos_label = 1, average = 'binary'))
print('RF-Hyp: ',recall_score(y_test, y_rf_best, pos_label = 1, average = 'binary'))

"""##Precission Score"""

print('DT: ',precision_score(y_test, y_dtree, pos_label = 1, average = 'binary'))
print('RF: ',precision_score(y_test, y_rf, pos_label = 1, average = 'binary'))
print('RF-Hyp: ',precision_score(y_test, y_rf_best, pos_label = 1, average = 'binary'))